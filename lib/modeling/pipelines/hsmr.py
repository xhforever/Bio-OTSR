# HSMR (Human Skeleton Mesh Recovery) 训练管道
# 实现基于Vision Transformer的人体姿态和形状恢复模型

from lib.kits.basic import *

from lib.utils.vis import Wis3D
from lib.utils.vis.py_renderer import render_mesh_overlay_img
from lib.utils.data import to_tensor
from lib.utils.media import draw_kp2d_on_img, annotate_img, splice_img
from lib.utils.camera import perspective_projection
from lib.body_models.abstract_skeletons import Skeleton_OpenPose25
from lib.modeling.losses import *
from lib.modeling.networks.discriminators import HSMRDiscriminator
from lib.platform.config_utils import get_PM_info_dict

# 导入局部细化分支模块
import sys
from pathlib import Path as PathLib
HSMR_ROOT = PathLib(__file__).parent.parent.parent.parent
sys.path.insert(0, str(HSMR_ROOT))
try:
    from local_refinement_branch import LocalRefinementBranch, PoseFusionModule
    LOCAL_BRANCH_AVAILABLE = True
except ImportError as e:
    LOCAL_BRANCH_AVAILABLE = False
    print(f"⚠ 局部细化分支未找到，将运行标准HSMR模式 (错误: {e})")


def build_inference_pipeline(
    model_root: Union[Path, str],
    ckpt_fn   : Optional[Union[Path, str]] = None,
    tuned_bcb : bool = True,
    device    : str = 'cpu',
):
    """
    构建HSMR推理管道
    
    Args:
        model_root: 模型根目录路径
        ckpt_fn: 检查点文件路径
        tuned_bcb: 是否使用调优的骨干网络
        device: 运行设备（cpu或gpu）
    
    Returns:
        配置好的HSMR推理管道
    """
    # 1.1. 加载配置文件
    if isinstance(model_root, str):
        model_root = Path(model_root)
    cfg_path = model_root / '.hydra' / 'config.yaml'
    cfg = OmegaConf.load(cfg_path)
    # 1.2. 覆盖PM信息字典
    PM_overrides = get_PM_info_dict()._pm_
    cfg._pm_ = PM_overrides
    get_logger(brief=True).info(f'Building inference pipeline of {cfg.exp_name}')

    # 2.1. 实例化管道
    init_bcb = not tuned_bcb
    pipeline = instantiate(cfg.pipeline, init_backbone=init_bcb, _recursive_=False)
    pipeline.set_data_adaption(data_module_name='IMG_PATCHES')
    # 2.2. 加载检查点
    if ckpt_fn is None:
        ckpt_fn = model_root / 'checkpoints' / 'hsmr.ckpt'
    pipeline.load_state_dict(torch.load(ckpt_fn, map_location=device)['state_dict'])
    get_logger(brief=True).info(f'Load checkpoint from {ckpt_fn}.')

    pipeline.eval()
    return pipeline.to(device)


class HSMRPipeline(pl.LightningModule):
    """
    HSMR训练管道主类
    
    基于PyTorch Lightning实现的人体姿态和形状恢复模型训练管道。
    包含骨干网络、预测头、损失函数和判别器等组件。
    """

    def __init__(self, cfg:DictConfig, name:str, init_backbone=True):
        """
        初始化HSMR训练管道
        
        Args:
            cfg: 配置文件
            name: 管道名称
            init_backbone: 是否初始化骨干网络
        """
        super(HSMRPipeline, self).__init__()
        self.name = name
        
        # 初始化模型组件
        self.skel_model = instantiate(cfg.SKEL)      # 人体骨骼模型（SMPL）
        self.backbone = instantiate(cfg.backbone)    # 骨干网络（Vision Transformer）
        self.head = instantiate(cfg.head)            # 预测头（输出姿态和形状参数）
        self.cfg = cfg

        if init_backbone:
            # 对于使用调优骨干网络检查点的推理模式，不需要在此初始化骨干网络
            self._init_backbone()

        # 初始化损失函数
        self.kp_3d_loss  = Keypoint3DLoss(loss_type='l1')  # 3D关键点损失
        self.kp_2d_loss  = Keypoint2DLoss(loss_type='l1')  # 2D关键点损失
        self.params_loss = ParameterLoss()                 # 参数损失

        # 初始化判别器（用于对抗训练）
        self.enable_disc = self.cfg.loss_weights.get('adversarial', 0) > 0
        if self.enable_disc:
            self.discriminator = HSMRDiscriminator()
            get_logger().warning(f'Discriminator enabled, the global_steps will be doubled. Use the checkpoints carefully.')
        else:
            self.discriminator = None
        
        # 初始化局部细化分支（如果配置中启用）
        self.enable_local_branch = LOCAL_BRANCH_AVAILABLE and cfg.get('enable_local_branch', False)
        if self.enable_local_branch:
            local_cfg = cfg.get('local_branch', {})
            self.local_branch = LocalRefinementBranch(
                crop_size=local_cfg.get('crop_size', 64),
                feature_dim=local_cfg.get('feature_dim', 128),
                ankle_joint_ids=(7, 8)
            )
            self.fusion_module = PoseFusionModule(
                ankle_joint_ids=(7, 8),
                fusion_weight_init=local_cfg.get('fusion_weight', 0.3),
                learnable_fusion_weight=local_cfg.get('learnable_fusion_weight', True)
            )
            get_logger().info(f'✓ 局部细化分支已启用 (crop_size={local_cfg.get("crop_size", 64)}, '
                            f'feature_dim={local_cfg.get("feature_dim", 128)}, '
                            f'fusion_weight={local_cfg.get("fusion_weight", 0.3)})')
        else:
            self.local_branch = None
            self.fusion_module = None
            if cfg.get('enable_local_branch', False) and not LOCAL_BRANCH_AVAILABLE:
                get_logger().warning('⚠ 配置中启用了局部分支但模块未找到，将运行标准HSMR模式')
            self.cfg.loss_weights.pop('adversarial', None)  # 如果未启用则移除对抗损失项

        # 手动控制优化过程，因为我们有对抗训练过程
        self.automatic_optimization = False
        self.set_data_adaption()

        # 用于可视化调试
        if False:
            self.wis3d = Wis3D(seq_name=PM.cfg.exp_name)
        else:
            self.wis3d = None

    def set_data_adaption(self, data_module_name:Optional[str]=None):
        if data_module_name is None:
            # get_logger().warning('Data adapter schema is not defined. The input will be regarded as image patches.')
            self.adapt_batch = self._adapt_img_inference
        elif data_module_name == 'IMG_PATCHES':
            self.adapt_batch = self._adapt_img_inference
        elif data_module_name.startswith('SKEL_HSMR_V1'):
            self.adapt_batch = self._adapt_hsmr_v1
        else:
            raise ValueError(f'Unknown data module: {data_module_name}')

    def print_summary(self, max_depth=1):
        from pytorch_lightning.utilities.model_summary.model_summary import ModelSummary
        print(ModelSummary(self, max_depth=max_depth))

    def configure_optimizers(self):
        optimizers = []

        params_main = filter(lambda p: p.requires_grad, self._params_main())
        optimizer_main = instantiate(self.cfg.optimizer, params=params_main)
        optimizers.append(optimizer_main)

        if len(self._params_disc()) > 0:
            params_disc = filter(lambda p: p.requires_grad, self._params_disc())
            optimizer_disc = instantiate(self.cfg.optimizer, params=params_disc)
            optimizers.append(optimizer_disc)

        return optimizers

    def training_step(self, raw_batch, batch_idx):
        """
        PyTorch Lightning训练步骤入口
        
        Args:
            raw_batch: 原始数据批次
            batch_idx: 批次索引
        """
        with PM.time_monitor('training_step'):
            return self._training_step(raw_batch, batch_idx)

    def _training_step(self, raw_batch, batch_idx):
        """
        执行一个训练步骤
        
        包括：
        1. 数据适配和预处理
        2. 前向传播预测
        3. 损失计算
        4. 反向传播和参数更新
        5. 判别器训练（可选）
        6. 日志记录和可视化
        """
        # GPU_monitor = GPUMonitor()
        # GPU_monitor.snapshot('HSMR training start')

        # 数据适配：将原始数据转换为模型所需的格式
        batch = self.adapt_batch(raw_batch['img_ds'])
        # GPU_monitor.snapshot('HSMR adapt batch')

        # 获取优化器（主模型和判别器）
        optimizers = self.optimizers(use_pl_optimizer=True)
        if isinstance(optimizers, List):
            optimizer_main, optimizer_disc = optimizers  # 主模型和判别器优化器
        else:
            optimizer_main = optimizers                  # 仅主模型优化器
        # GPU_monitor.snapshot('HSMR get optimizer')

        # 1. 主模型前向传播
        with PM.time_monitor('forward_step'):
            img_patch = to_tensor(batch['img_patch'], self.device)  # 输入图像块 (B, C, H, W)
            B = len(img_patch)  # 批次大小
            outputs = self.forward_step(img_patch)  # 模型预测输出
            # GPU_monitor.snapshot('HSMR forward')
            pd_skel_params = HSMRPipeline._adapt_skel_params(outputs['pd_params'])  # 适配骨骼参数格式
            # GPU_monitor.snapshot('HSMR adapt SKEL params')

        # 2. [可选] 在主训练步骤中进行判别器前向传播
        if self.enable_disc:
            with PM.time_monitor('disc_forward'):
                # 将姿态参数转换为旋转矩阵形式
                pd_poses_mat, _ = self.skel_model.pose_params_to_rot(pd_skel_params['poses'])  # (B, J=24, 3, 3)
                pd_poses_body_mat = pd_poses_mat[:, 1:, :, :]  # 去除根关节，保留身体姿态 (B, J=23, 3, 3)
                pd_betas = pd_skel_params['betas']  # 人体形状参数 (B, 10)
                # 判别器前向传播，判断预测的姿态是否真实
                disc_out = self.discriminator(
                        poses_body = pd_poses_body_mat,   # 身体姿态矩阵 (B, J=23, 3, 3)
                        betas      = pd_betas,            # 形状参数 (B, 10)
                    )
        else:
            disc_out = None  # 未启用判别器

        # 3. 准备辅助产品（用于损失计算和可视化）
        with PM.time_monitor('Secondary Products Preparation'):
            # 3.1. 人体模型输出：从姿态参数生成关键点和网格
            with PM.time_monitor('SKEL Forward'):
                skel_outputs = self.skel_model(**pd_skel_params, skelmesh=False)
                pd_kp3d = skel_outputs.joints      # 预测的3D关键点 (B, Q=44, 3)
                pd_skin = skel_outputs.skin_verts  # 预测的人体网格顶点 (B, V=6890, 3)
            # 3.2. 将3D关键点重投影到2D平面
            with PM.time_monitor('Reprojection'):
                pd_kp2d = perspective_projection(
                        points       = pd_kp3d,                                                    # 3D关键点 (B, K=Q=44, 3)
                        translation  = outputs['pd_cam_t'],                                        # 相机平移 (B, 3)
                        focal_length = outputs['focal_length'] / self.cfg.policy.img_patch_size,  # 标准化焦距 (B, 2)
                    )  # 预测的2D关键点 (B, 44, 2)
            # 3.3. 从输入中提取真实标签
            gt_kp2d_with_conf = batch['kp2d'].clone()  # (B, 44, 3)
            gt_kp3d_with_conf = batch['kp3d'].clone()  # (B, 44, 4)
            # 3.4. 仅提取真实标签皮肤网格用于可视化
            gt_skel_params = HSMRPipeline._adapt_skel_params(batch['gt_params'])  # {poses, betas}
            gt_skel_params = {k: v[:self.cfg.logger.samples_per_record] for k, v in gt_skel_params.items()}
            skel_outputs = self.skel_model(**gt_skel_params, skelmesh=False)
            gt_skin = skel_outputs.skin_verts  # (B', V=6890, 3)
            gt_valid_body = batch['has_gt_params']['poses_body'][:self.cfg.logger.samples_per_record]  # {poses_orient, poses_body, betas}
            gt_valid_orient = batch['has_gt_params']['poses_orient'][:self.cfg.logger.samples_per_record]  # {poses_orient, poses_body, betas}
            gt_valid_betas = batch['has_gt_params']['betas'][:self.cfg.logger.samples_per_record]  # {poses_orient, poses_body, betas}
            gt_valid = torch.logical_and(torch.logical_and(gt_valid_body, gt_valid_orient), gt_valid_betas)
        # GPU_monitor.snapshot('HSMR secondary products')

        # 4. 计算损失
        with PM.time_monitor('Compute Loss'):
            loss_main, losses_main = self._compute_losses_main(
                    self.cfg.loss_weights,
                    pd_kp3d,  # (B, 44, 3)
                    gt_kp3d_with_conf,  # (B, 44, 4)
                    pd_kp2d,  # (B, 44, 2)
                    gt_kp2d_with_conf,  # (B, 44, 3)
                    outputs['pd_params'],  # {'poses_orient':..., 'poses_body':..., 'betas':...}
                    batch['gt_params'],  # {'poses_orient':..., 'poses_body':..., 'betas':...}
                    batch['has_gt_params'],
                    disc_out,
                )
        # GPU_monitor.snapshot('HSMR compute losses')
        if torch.isnan(loss_main):
            get_logger().error(f'NaN detected in loss computation. Losses: {losses}')

        # 5. 主模型反向传播
        with PM.time_monitor('Backward Step'):
            optimizer_main.zero_grad()
            self.manual_backward(loss_main)
            optimizer_main.step()
        # GPU_monitor.snapshot('HSMR backwards')

        # 6. [可选] 判别器训练部分
        if self.enable_disc:
            with PM.time_monitor('Train Discriminator'):
                losses_disc = self._train_discriminator(
                        mocap_batch       = raw_batch['mocap_ds'],
                        pd_poses_body_mat = pd_poses_body_mat,
                        pd_betas          = pd_betas,
                        optimizer         = optimizer_disc,
                    )
        else:
            losses_disc = {}

        # 7. 日志记录
        with PM.time_monitor('Tensorboard Logging'):
            vis_data = {
                    'img_patch'         : to_numpy(img_patch[:self.cfg.logger.samples_per_record]).transpose((0, 2, 3, 1)).copy(),
                    'pd_kp2d'           : pd_kp2d[:self.cfg.logger.samples_per_record].clone(),
                    'pd_kp3d'           : pd_kp3d[:self.cfg.logger.samples_per_record].clone(),
                    'gt_kp2d_with_conf' : gt_kp2d_with_conf[:self.cfg.logger.samples_per_record].clone(),
                    'gt_kp3d_with_conf' : gt_kp3d_with_conf[:self.cfg.logger.samples_per_record].clone(),
                    'pd_skin'           : pd_skin[:self.cfg.logger.samples_per_record].clone(),
                    'gt_skin'           : gt_skin.clone(),
                    'gt_skin_valid'     : gt_valid,
                    'cam_t'             : outputs['pd_cam_t'][:self.cfg.logger.samples_per_record].clone(),
                    'img_key'           : batch['__key__'][:self.cfg.logger.samples_per_record],
                }
            self._tb_log(losses_main=losses_main, losses_disc=losses_disc, vis_data=vis_data)
        # GPU_monitor.snapshot('HSMR logging')
        self.log('_/loss_main', losses_main['weighted_sum'], on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=B)

        # GPU_monitor.report_all()
        return outputs

    def forward(self, batch):
        '''
        ### 返回值
        - outputs: Dict
            - pd_kp3d: torch.Tensor, shape (B, Q=44, 3)
            - pd_kp2d: torch.Tensor, shape (B, Q=44, 2)
            - pred_keypoints_2d: torch.Tensor, shape (B, Q=44, 2)
            - pred_keypoints_3d: torch.Tensor, shape (B, Q=44, 3)
            - pd_params: Dict
                - poses: torch.Tensor, shape (B, 46)
                - betas: torch.Tensor, shape (B, 10)
            - pd_cam: torch.Tensor, shape (B, 3)
            - pd_cam_t: torch.Tensor, shape (B, 3)
            - focal_length: torch.Tensor, shape (B, 2)
        '''
        batch = self.adapt_batch(batch)

        # 1. 主模型前向传播
        img_patch = to_tensor(batch['img_patch'], self.device)  # (B, C, H, W)
        outputs = self.forward_step(img_patch)  # {...}

        # 2. 准备辅助产品
        # 2.1. 人体模型输出
        pd_skel_params = HSMRPipeline._adapt_skel_params(outputs['pd_params'])
        skel_outputs = self.skel_model(**pd_skel_params, skelmesh=False)
        pd_kp3d = skel_outputs.joints  # (B, Q=44, 3)
        pd_skin_verts = skel_outputs.skin_verts.detach().cpu().clone()  # (B, V=6890, 3)
        # 2.2. 将3D关节重投影到2D平面
        pd_kp2d = perspective_projection(
                points       = to_tensor(pd_kp3d, device=self.device),  # (B, K=Q=44, 3)
                translation  = to_tensor(outputs['pd_cam_t'], device=self.device),  # (B, 3)
                focal_length = to_tensor(outputs['focal_length'], device=self.device) / self.cfg.policy.img_patch_size,  # (B, 2)
            )

        outputs['pd_kp3d'] = pd_kp3d
        outputs['pd_kp2d'] = pd_kp2d
        outputs['pred_keypoints_2d'] = pd_kp2d  # 适配HMR2.0的脚本
        outputs['pred_keypoints_3d'] = pd_kp3d  # 适配HMR2.0的脚本
        outputs['pd_params'] = pd_skel_params
        outputs['pd_skin_verts'] = pd_skin_verts

        return outputs

    def forward_step(self, x:torch.Tensor, original_img:Optional[torch.Tensor]=None):
        '''
        在模型上运行推理步骤

        ### 参数
        - x: torch.Tensor, shape (B, C, H, W)
            - 输入图像块
        - original_img: Optional[torch.Tensor]
            - 原始高分辨率图像（用于局部分支ROI裁剪）

        ### 返回值
        - outputs: Dict
            - 'pd_cam': torch.Tensor, shape (B, 3)
                - 预测的相机参数
            - 'pd_params': Dict
                - 预测的人体模型参数
            - 'focal_length': float
        '''
        # GPU_monitor = GPUMonitor()
        B = len(x)

        # 1. 从图像中提取特征
        #    输入尺寸为256*256，但ViT需要256*192。TODO: 使这更优雅
        with PM.time_monitor('Backbone Forward'):
            feats = self.backbone(x[:, :, :, 32:-32])
        # GPU_monitor.snapshot('HSMR forward backbone')


        # 2. 运行预测头来预测人体模型参数
        with PM.time_monitor('Predict Head Forward'):
            pd_params, pd_cam = self.head(feats)
        # GPU_monitor.snapshot('HSMR forward head')

        # 3. 将相机参数转换为相机平移
        focal_length = self.cfg.policy.focal_length * torch.ones(B, 2, device=self.device, dtype=pd_cam.dtype)  # (B, 2)
        pd_cam_t = torch.stack([
                    pd_cam[:, 1],
                    pd_cam[:, 2],
                    2 * focal_length[:, 0] / (self.cfg.policy.img_patch_size * pd_cam[:, 0] + 1e-9)
                ], dim=-1)  # (B, 3)

        # 4. 存储结果
        outputs = {
                'pd_cam'       : pd_cam,
                'pd_cam_t'     : pd_cam_t,
                'pd_params'    : pd_params,
                # 'pd_params'    : {k: v.clone() for k, v in pd_params.items()},
                'focal_length' : focal_length,  # (B, 2)
            }
        
        # 5. 局部细化分支（如果启用）
        if self.enable_local_branch and self.local_branch is not None:
            with PM.time_monitor('Local Branch'):
                # 5.1. 获取2D关键点用于定位踝关节
                pd_skel_params = HSMRPipeline._adapt_skel_params(pd_params)
                skel_outputs = self.skel_model(**pd_skel_params, skelmesh=False)
                pd_kp3d = skel_outputs.joints  # (B, 44, 3)
                
                # 投影到2D
                pd_kp2d = perspective_projection(
                    points=pd_kp3d,
                    translation=pd_cam_t,
                    focal_length=focal_length / self.cfg.policy.img_patch_size
                )  # (B, 44, 2)
                
                # 转换为图像坐标
                img_for_crop = original_img if original_img is not None else x
                H, W = img_for_crop.shape[2], img_for_crop.shape[3]
                pd_kp2d_img = pd_kp2d.clone()
                pd_kp2d_img[..., 0] = (pd_kp2d[..., 0] + 1) / 2 * W
                pd_kp2d_img[..., 1] = (pd_kp2d[..., 1] + 1) / 2 * H
                
                # 5.2. 局部分支预测残差
                local_outputs = self.local_branch(img_for_crop, pd_kp2d_img, which_ankle='both')
                
                # 5.3. 融合姿态
                global_poses = pd_params['poses']  # (B, 46)
                fused_poses = self.fusion_module(global_poses, local_outputs)
                
                # 5.4. 更新输出参数
                outputs['pd_params_global'] = pd_params  # 保存主路原始预测
                outputs['pd_params'] = {
                    'poses': fused_poses,
                    'poses_orient': fused_poses[:, :3],
                    'poses_body': fused_poses[:, 3:],
                    'betas': pd_params['betas']
                }
                outputs['local_residuals'] = local_outputs
                outputs['fusion_weight'] = self.fusion_module.fusion_weight.item()
        
        # GPU_monitor.report_all()
        return outputs


    # ========== 内部函数 ==========

    def _params_main(self):
        params = list(self.head.parameters()) + list(self.backbone.parameters())
        # 包含局部分支参数（如果启用）
        if self.enable_local_branch and self.local_branch is not None:
            params += list(self.local_branch.parameters())
            params += list(self.fusion_module.parameters())
        return params

    def _params_disc(self):
        if self.discriminator is None:
            return []
        else:
            return list(self.discriminator.parameters())

    @staticmethod
    def _adapt_skel_params(params:Dict):
        ''' 将参数格式从 [pose_orient, pose_body, betas, trans] 改为 [poses, betas, trans] '''
        adapted_params = {}

        if 'poses' in params.keys():
            adapted_params['poses'] = params['poses']
        elif 'poses_orient' in params.keys() and 'poses_body' in params.keys():
            poses_orient = params['poses_orient']  # (B, 3)
            poses_body = params['poses_body']  # (B, 43)
            adapted_params['poses'] = torch.cat([poses_orient, poses_body], dim=1)  # (B, 46)
        else:
            raise ValueError(f'Cannot find the poses parameters among {list(params.keys())}.')

        if 'betas' in params.keys():
            adapted_params['betas'] = params['betas']  # (B, 10)
        else:
            raise ValueError(f'Cannot find the betas parameters among {list(params.keys())}.')

        return adapted_params

    def _init_backbone(self):
        # 1. 加载骨干网络权重
        get_logger().info(f'Loading backbone weights from {self.cfg.backbone_ckpt}')
        state_dict = torch.load(self.cfg.backbone_ckpt, map_location='cpu')['state_dict']
        if 'backbone.cls_token' in state_dict.keys():
            state_dict = {k: v for k, v in state_dict.items() if 'backbone' in k and 'cls_token' not in k}
            state_dict = {k.replace('backbone.', ''): v for k, v in state_dict.items()}
        missing, unexpected = self.backbone.load_state_dict(state_dict)
        if len(missing) > 0:
            get_logger().warning(f'Missing keys in backbone: {missing}')
        if len(unexpected) > 0:
            get_logger().warning(f'Unexpected keys in backbone: {unexpected}')

        # 2. 如果需要则冻结骨干网络
        if self.cfg.get('freeze_backbone', False):
            self.backbone.eval()
            self.backbone.requires_grad_(False)

    def _compute_losses_main(
        self,
        loss_weights : Dict,
        pd_kp3d      : torch.Tensor,
        gt_kp3d      : torch.Tensor,
        pd_kp2d      : torch.Tensor,
        gt_kp2d      : torch.Tensor,
        pd_params    : Dict,
        gt_params    : Dict,
        has_params   : Dict,
        disc_out     : Optional[torch.Tensor]=None,
        *args, **kwargs,
    ) -> Tuple[torch.Tensor, Dict]:
        ''' 根据配置文件计算加权损失 '''

        # 1. 准备阶段
        with PM.time_monitor('Preparation'):
            B = len(pd_kp3d)
            gt_skel_params = HSMRPipeline._adapt_skel_params(gt_params)  # {poses, betas}
            pd_skel_params = HSMRPipeline._adapt_skel_params(pd_params)  # {poses, betas}

            gt_betas = gt_skel_params['betas'].reshape(-1, 10)
            pd_betas = pd_skel_params['betas'].reshape(-1, 10)
            gt_poses = gt_skel_params['poses'].reshape(-1, 46)
            pd_poses = pd_skel_params['poses'].reshape(-1, 46)

        # 2. 关键点损失
        with PM.time_monitor('kp2d & kp3d Loss'):
            kp2d_loss = self.kp_2d_loss(pd_kp2d, gt_kp2d) / B
            kp3d_loss = self.kp_3d_loss(pd_kp3d, gt_kp3d) / B

        # 3. 先验损失
        with PM.time_monitor('Prior Loss'):
            prior_loss = compute_poses_angle_prior_loss(pd_poses).mean()  # (,)

        # 4. 参数损失
        if self.cfg.sp_poses_repr == 'rotation_matrix':
            with PM.time_monitor('q2mat'):
                gt_poses_mat, _ = self.skel_model.pose_params_to_rot(gt_poses)  # (B, J=24, 3, 3)
                pd_poses_mat, _ = self.skel_model.pose_params_to_rot(pd_poses)  # (B, J=24, 3, 3)

                gt_poses = gt_poses_mat.reshape(-1, 24*3*3)  # (B, 24*3*3)
                pd_poses = pd_poses_mat.reshape(-1, 24*3*3)  # (B, 24*3*3)

        with PM.time_monitor('Parameters Loss'):
            poses_orient_loss = self.params_loss(pd_poses[:, :9], gt_poses[:, :9], has_params['poses_orient']) / B
            poses_body_loss   = self.params_loss(pd_poses[:, 9:], gt_poses[:, 9:], has_params['poses_body']) / B
            betas_loss        = self.params_loss(pd_betas, gt_betas, has_params['betas']) / B

        # 5. 收集主要损失
        with PM.time_monitor('Accumulate'):
            losses = {
                    'kp3d'         : kp3d_loss,          # (,)
                    'kp2d'         : kp2d_loss,          # (,)
                    'prior'        : prior_loss,         # (,)
                    'poses_orient' : poses_orient_loss,  # (,)
                    'poses_body'   : poses_body_loss,    # (,)
                    'betas'        : betas_loss,         # (,)
                }

        # 6. 考虑对抗损失
        if disc_out is not None:
            with PM.time_monitor('Adversarial Loss'):
                adversarial_loss = ((disc_out - 1.0) ** 2).sum() / B  # (,)
                losses['adversarial'] = adversarial_loss

        with PM.time_monitor('Accumulate'):
            loss = torch.tensor(0., device=self.device)
            for k, v in losses.items():
                loss += v * loss_weights[k]
            losses = {k: v.item() for k, v in losses.items()}
            losses['weighted_sum'] = loss.item()
        return loss, losses

    def _train_discriminator(self, mocap_batch, pd_poses_body_mat, pd_betas, optimizer):
        '''
        使用回归的人体模型参数和真实的动作捕捉数据训练判别器

        ### 参数
        - mocap_batch: Dict
            - 'poses_body': torch.Tensor, shape (B, 43)
            - 'betas': torch.Tensor, shape (B, 10)
        - pd_poses_body_mat: torch.Tensor, shape (B, J=23, 3, 3)
        - pd_betas: torch.Tensor, shape (B, 10)
        - optimizer: torch.optim.Optimizer

        ### 返回值
        - losses: Dict
            - 'pd_disc': float
            - 'mc_disc': float
        '''
        pd_B = len(pd_poses_body_mat)
        mc_B = len(mocap_batch['poses_body'])
        get_logger().warning(f'pd_B: {pd_B} != mc_B: {mc_B}')

        # 1. 提取真实的3D动作捕捉标签
        mc_poses_body = mocap_batch['poses_body']  # (B, 43)
        padding_zeros = mc_poses_body.new_zeros(mc_B, 3)  # (B, 3)
        mc_poses = torch.cat([padding_zeros, mc_poses_body], dim=1)  # (B, 46)
        mc_poses_mat, _ = self.skel_model.pose_params_to_rot(mc_poses)  # (B, J=24, 3, 3)
        mc_poses_body_mat = mc_poses_mat[:, 1:, :, :]  # (B, J=23, 3, 3)
        mc_betas = mocap_batch['betas']  # (B, 10)

        # 2. 前向传播
        # 对预测数据进行判别器前向传播
        pd_disc_out = self.discriminator(pd_poses_body_mat.detach(), pd_betas.detach())
        pd_disc_loss = ((pd_disc_out - 0.0) ** 2).sum() / pd_B  # (,)
        # 对真实动作捕捉数据进行判别器前向传播
        mc_disc_out = self.discriminator(mc_poses_body_mat, mc_betas)
        mc_disc_loss = ((mc_disc_out - 1.0) ** 2).sum() / pd_B  # (,)  TODO: 这个'pd_B'来自HMR2，不确定是否是bug

        # 3. 反向传播
        disc_loss = self.cfg.loss_weights.adversarial * (pd_disc_loss + mc_disc_loss)
        optimizer.zero_grad()
        self.manual_backward(disc_loss)
        optimizer.step()

        return {
                'pd_disc': pd_disc_loss.item(),
                'mc_disc': mc_disc_loss.item(),
            }

    @rank_zero_only
    def _tb_log(self, losses_main:Dict, losses_disc:Dict, vis_data:Dict, mode:str='train'):
        ''' 将日志信息写入TensorBoard '''
        if self.logger is None:
            return

        if self.global_step != 1 and self.global_step % self.cfg.logger.interval != 0:
            return

        # 1. 损失
        summary_writer = self.logger.experiment
        for loss_name, loss_val in losses_main.items():
            summary_writer.add_scalar(f'{mode}/losses_main/{loss_name}', loss_val, self.global_step)
        for loss_name, loss_val in losses_disc.items():
            summary_writer.add_scalar(f'{mode}/losses_disc/{loss_name}', loss_val, self.global_step)

        # 2. 可视化
        try:
            pelvis_id = 39
            # 2.1. 可视化3D信息
            self.wis3d.add_motion_mesh(
                verts = vis_data['pd_skin'] - vis_data['pd_kp3d'][:, pelvis_id:pelvis_id+1],  # 将网格居中
                faces = self.skel_model.skin_f,
                name  = 'pd_skin',
            )
            self.wis3d.add_motion_mesh(
                verts = vis_data['gt_skin'] - vis_data['gt_kp3d_with_conf'][:, pelvis_id:pelvis_id+1, :3],  # 将网格居中
                faces = self.skel_model.skin_f,
                name  = 'gt_skin',
            )
            self.wis3d.add_motion_skel(
                joints = vis_data['pd_kp3d'] - vis_data['pd_kp3d'][:, pelvis_id:pelvis_id+1],
                bones  = Skeleton_OpenPose25.bones,
                colors = Skeleton_OpenPose25.bone_colors,
                name   = 'pd_kp3d',
            )

            aligned_gt_kp3d = vis_data['gt_kp3d_with_conf']
            aligned_gt_kp3d[..., :3] -= vis_data['gt_kp3d_with_conf'][:, pelvis_id:pelvis_id+1, :3]
            self.wis3d.add_motion_skel(
                joints = aligned_gt_kp3d,
                bones  = Skeleton_OpenPose25.bones,
                colors = Skeleton_OpenPose25.bone_colors,
                name   = 'gt_kp3d',
            )
        except Exception as e:
            if self.wis3d is not None:
                get_logger().error(f'Failed to visualize the current performance on wis3d: {e}')

        try:
            # 2.2. 可视化2D信息
            if vis_data['img_patch'] is not None:
                # 在原始图像上叠加结果的皮肤网格
                imgs_spliced = []
                for i, img_patch in enumerate(vis_data['img_patch']):
                    # TODO: 使这更优雅
                    img_mean = to_numpy(OmegaConf.to_container(self.cfg.policy.img_mean))[None, None]  # (1, 1, 3)
                    img_std = to_numpy(OmegaConf.to_container(self.cfg.policy.img_std))[None, None]  # (1, 1, 3)
                    img_patch = ((img_mean + img_patch * img_std) * 255).astype(np.uint8)

                    img_patch_raw = annotate_img(img_patch, 'raw')

                    img_with_mesh = render_mesh_overlay_img(
                            faces      = self.skel_model.skin_f,
                            verts      = vis_data['pd_skin'][i].float(),
                            K4         = [self.cfg.policy.focal_length, self.cfg.policy.focal_length, 128, 128],
                            img        = img_patch,
                            Rt         = [torch.eye(3).float(), vis_data['cam_t'][i].float()],
                            mesh_color = 'pink',
                        )
                    img_with_mesh = annotate_img(img_with_mesh, 'pd_mesh')

                    img_with_gt_mesh = render_mesh_overlay_img(
                            faces      = self.skel_model.skin_f,
                            verts      = vis_data['gt_skin'][i].float(),
                            K4         = [self.cfg.policy.focal_length, self.cfg.policy.focal_length, 128, 128],
                            img        = img_patch,
                            Rt         = [torch.eye(3).float(), vis_data['cam_t'][i].float()],
                            mesh_color = 'pink',
                        )
                    valid = 'valid' if vis_data['gt_skin_valid'][i] else 'invalid'
                    img_with_gt_mesh = annotate_img(img_with_gt_mesh, f'gt_mesh_{valid}')

                    img_with_gt = annotate_img(img_patch, 'gt_kp2d')
                    gt_kp2d_with_conf = vis_data['gt_kp2d_with_conf'][i]
                    gt_kp2d_with_conf[:, :2] = (gt_kp2d_with_conf[:, :2] + 0.5) * self.cfg.policy.img_patch_size
                    img_with_gt = draw_kp2d_on_img(
                            img_with_gt,
                            gt_kp2d_with_conf,
                            Skeleton_OpenPose25.bones,
                            Skeleton_OpenPose25.bone_colors,
                        )

                    img_with_pd = annotate_img(img_patch, 'pd_kp2d')
                    pd_kp2d_vis = vis_data['pd_kp2d'][i]
                    pd_kp2d_vis = (pd_kp2d_vis + 0.5) * self.cfg.policy.img_patch_size
                    img_with_pd = draw_kp2d_on_img(
                            img_with_pd,
                            (vis_data['pd_kp2d'][i] + 0.5) * self.cfg.policy.img_patch_size,
                            Skeleton_OpenPose25.bones,
                            Skeleton_OpenPose25.bone_colors,
                        )

                    img_spliced = splice_img([img_patch_raw, img_with_gt, img_with_pd, img_with_mesh, img_with_gt_mesh], grid_ids=[[0, 1, 2, 3, 4]])
                    img_spliced = annotate_img(img_spliced, vis_data['img_key'][i], pos='tl')
                    imgs_spliced.append(img_spliced)

                    try:
                        self.wis3d.set_scene_id(i)
                        self.wis3d.add_image(
                            image = img_spliced,
                            name = 'image',
                        )
                    except Exception as e:
                        if self.wis3d is not None:
                            get_logger().error(f'Failed to visualize the current performance on wis3d: {e}')

                img_final = splice_img(imgs_spliced, grid_ids=[[i] for i in range(len(vis_data['img_patch']))])

                img_final = to_tensor(img_final, device=None).permute(2, 0, 1)
                summary_writer.add_image(f'{mode}/visualization', img_final, self.global_step)

        except Exception as e:
            get_logger().error(f'Failed to visualize the current performance: {e}')


    def _adapt_hsmr_v1(self, batch):
        from lib.data.augmentation.skel import rot_skel_on_plane
        rot_deg = batch['augm_args']['rot_deg']  # (B,)

        skel_params = rot_skel_on_plane(batch['raw_skel_params'], rot_deg)
        batch['gt_params'] = {}
        batch['gt_params']['poses_orient'] = skel_params['poses'][:, :3]
        batch['gt_params']['poses_body'] = skel_params['poses'][:, 3:]
        batch['gt_params']['betas'] = skel_params['betas']

        has_skel_params = batch['has_skel_params']
        batch['has_gt_params'] = {}
        batch['has_gt_params']['poses_orient'] = has_skel_params['poses']
        batch['has_gt_params']['poses_body'] = has_skel_params['poses']
        batch['has_gt_params']['betas'] = has_skel_params['betas']
        return batch

    def _adapt_img_inference(self, img_patches):
        return {'img_patch': img_patches}