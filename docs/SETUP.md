# Environment-setup

## Conda Installation
The code was tested on pytorch 2.0 + cu117, xfromers==0.0.18, python=3.9.23, you may need to carefully adjust the version of torch, xfromers and cuda according to your GPUs.
```
conda env create -f environment.yml
```
```
conda activate skelvit
```
```
pip install --no-deps albucore==0.0.24
```
## Detectron2 Installation
To test images â€œin the wild,â€ you need to install the appropriate version of [Detectron2](https://detectron2.readthedocs.io/en/latest/tutorials/install.html) corresponding to your specific GPU, CUDA, and PyTorch configuration.

## body_models Installation
Due to copyright considerations, we cannot directly provide files/folders in [body_models](https://github.com/Intellindust-AI-Lab/SKEL-CF/tree/main/body_models), you should get skel folder comes from [SKEL](https://github.com/MarilynKeller/SKEL/tree/master/skel) and skel_utils folder and the remaining related code from [HSMR](https://github.com/IsshikiHugh/HSMR/tree/main/lib/body_models) and put these folders/files into the corresponding locations as shown below. 

Some external code and libraries include dependencies on and calls to their original repositories, so you may need to adjust these parts yourself (for example, the issue encountered in [issue#5](https://github.com/Intellindust-AI-Lab/SKEL-CF/issues/5)). If you run into any problems during the installation process, please feel free to raise them in an issue.

    body_models/
    â”œâ”€â”€ skel/
    â”‚   â”œâ”€â”€ alignment/
    â”‚   â”‚   â”œâ”€â”€ align_config_joint.py
    â”‚   â”‚   â”œâ”€â”€ align_config.py
    â”‚   â”‚   â”œâ”€â”€ aligner.py
    â”‚   â”‚   â”œâ”€â”€ losses.py
    â”‚   â”‚   â””â”€â”€ utils.py
    â”‚   â””â”€â”€ fit_osim/
    â”‚       â”œâ”€â”€ mappings/
    â”‚       â”œâ”€â”€ Fitting_SKEL_to_osim.md
    â”‚       â”œâ”€â”€ osim_aug.py
    â”‚       â””â”€â”€ osim_fitter.py
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ config.py
    â”œâ”€â”€ joints_def.py
    â”œâ”€â”€ kin_skel.py
    â”œâ”€â”€ osim_rot.py
    â”œâ”€â”€ skel_model.py
    â”œâ”€â”€ utils.py
    â”œâ”€â”€ skel_utils/
    â”‚   â”œâ”€â”€ augmentation.py
    â”‚   â”œâ”€â”€ definition.py
    â”‚   â”œâ”€â”€ limits.py
    â”‚   â””â”€â”€ transforms.py
    â””â”€â”€ skel_wrapper.py

# Data-preparation

Choose your setup path:
- [Quick Start](#-quick-start): Run the demo in minutes with pre-configured settings
- [Advanced Setup](#-Advanced-setup): Deep dive into the method details and custom configurations

<!-- We have uploaded our various datasets to the OneDrive. You can download the required resources from [this link](https://1drv.ms/f/c/89cf0bfd859af8e2/IgCPQSPYHWA1T495bGcBbJCqAVPl4MX0-1-_8G4LdbYRxSA?e=8JHYI7). -->
## ğŸš€ Quick start

### Prerequisites: Download Body Models

Before running the code, you need to download these files:

| Model | Description | Download Link |
|-------|-------------|---------------|
| SKEL | Skeleton body model | [Download](https://skel.is.tue.mpg.de/download.php) |
| J_regressor | Auxiliary joint regressor | [Download](https://onedrive.live.com/?redeem=aHR0cHM6Ly8xZHJ2Lm1zL2YvYy84OWNmMGJmZDg1OWFmOGUyL0lnQ1BRU1BZSFdBMVQ0OTViR2NCYkpDcUFWUGw0TVgwLTEtXzhHNExkYllSeFNBP2U9OEpIWUk3&id=89CF0BFD859AF8E2%21sd823418f601d4f358f796c67016c90aa&cid=89CF0BFD859AF8E2) |
| SMPL | SMPL body model | [Download](https://smpl.is.tue.mpg.de/download.php) |

Once downloaded, organize the files according to the directory structure below:

    data_inputs/
    â””â”€â”€ body_models/
        â”œâ”€â”€ skel/
        â”‚   â”œâ”€â”€ Geometry/
        â”‚   â”œâ”€â”€ sample_motion/
        â”‚   â”œâ”€â”€ bsm.osim
        â”‚   â”œâ”€â”€ changelog_v1.1.1.txt
        â”‚   â”œâ”€â”€ skel_female.pkl
        â”‚   â”œâ”€â”€ skel_male.pkl
        â”‚   â””â”€â”€ tmp.osim
        â”œâ”€â”€ SMPL/
        â”‚   â”œâ”€â”€ SMPL_FEMALE.pkl
        â”‚   â”œâ”€â”€ SMPL_MALE.pkl
        â”‚   â””â”€â”€ SMPL_NEUTRAL.pkl
        â”œâ”€â”€ J_regressor_h36m.pkl
        â”œâ”€â”€ SMPL_TO_J19.pkl
        â”œâ”€â”€ J_regressor_SKEL_mix_MALE.pkl
        â””â”€â”€ J_regressor_SMPL_MALE.pkl

### SKEL-CF Checkpoint
Download **SKEL-CF.pth** from [SKEL-CF](https://onedrive.live.com/?redeem=aHR0cHM6Ly8xZHJ2Lm1zL2YvYy84OWNmMGJmZDg1OWFmOGUyL0lnQ1BRU1BZSFdBMVQ0OTViR2NCYkpDcUFWUGw0TVgwLTEtXzhHNExkYllSeFNBP2U9OEpIWUk3&id=89CF0BFD859AF8E2%21sd823418f601d4f358f796c67016c90aa&cid=89CF0BFD859AF8E2) and place it at data_outputs/exp/ (or wherever you like, but rember to adjust the path in eval.sh and rendering bash shells)

## âš™ï¸ Advanced setup

### Pretrained Camera Model Checkpoint & ViTPose Backbone Checkpoint
You should download the pretrained checkpoint " cam_model_cleaned.ckpt" from the [CameraHMR](https://camerahmr.is.tue.mpg.de), it's called "HumanFov Model" on their website, with a size about 768 MB. Also, you should download vitpose_backbone.pth from [VITPOSE](https://github.com/ViTAE-Transformer/ViTPose) with simple decoder (about 2.35GB), we use vitpose-H as baseline, you can also choose S/B/L model if you don't have enough VRAM, and place them under the data_inputs/backone directory. The file structure will be as follows:

    data_inputs/
    â””â”€â”€ backbone/
        â”œâ”€â”€ cam_model_cleaned.ckpt
        â””â”€â”€ vitpose_backbone.pth


### Training images
1. Download the training datasets following the [HSMR](https://github.com/IsshikiHugh/HSMR) instructions.
2. Extract webdataset archives into individual image files.
3. Organize the images according to the directory structure shown below.

```shell
    data_inputs/
    â””â”€â”€ skel-training-images/
        â”œâ”€â”€ aic-train
        â”œâ”€â”€ coco-train
        â”œâ”€â”€ h36m-train
        â”œâ”€â”€ insta-train
        â”œâ”€â”€ mpi-inf-train
        â””â”€â”€ mpii-train
```
### Training labels
Download `skel-training-labels` folder from OneDrive and place it under the `data_inputs/` directory. The file structure should be:

```shell
    data_inputs/
    â””â”€â”€ skel-training-labels/
        â”œâ”€â”€ aic-release_skel.npz
        â”œâ”€â”€ coco-release_skel.npz
        â”œâ”€â”€ h36m-release_skel.npz
        â”œâ”€â”€ insta1-release_skel.npz
        â”œâ”€â”€ insta2-release_skel.npz
        â”œâ”€â”€ mpi-inf-release_skel.npz
        â””â”€â”€ mpii-release_skel.npz
```

# Evaluation Data
We evaluated our method on several benchmark datasets, including COCO, 3DPW, EMDB, SPEC-SYN and H36M used by [HSMR](https://github.com/IsshikiHugh/HSMR) or [CameraHMR](https://camerahmr.is.tue.mpg.de/). We also tested on the yoga action dataset [MOYO](https://moyo.is.tue.mpg.de/), as well as its more challenging subset, MOYO-HARD. Some datasets are relatively difficult to process, so just take your time.

## Evaluation labels
Download the evaluation labels from [SKEL-CF](https://onedrive.live.com/?redeem=aHR0cHM6Ly8xZHJ2Lm1zL2YvYy84OWNmMGJmZDg1OWFmOGUyL0lnQ1BRU1BZSFdBMVQ0OTViR2NCYkpDcUFWUGw0TVgwLTEtXzhHNExkYllSeFNBP2U9OEpIWUk3&id=89CF0BFD859AF8E2%21sd823418f601d4f358f796c67016c90aa&cid=89CF0BFD859AF8E2)
```shell
    data_inputs/
    â””â”€â”€ skel-evaluation-labels/
        â”œâ”€â”€ moyo_hard.npz
        â”œâ”€â”€ moyo_v2.npz
        â”œâ”€â”€ h36m_val_p2.npz
        â”œâ”€â”€ coco_val.npz
        â”œâ”€â”€ spec_test.npz
        â”œâ”€â”€ 3dpw_test.npz
        â””â”€â”€ emdb_test.npz
```    

## Evaluation Images
Download the evaluation images from the following sources:
- [3DPW](https://virtualhumans.mpi-inf.mpg.de/3DPW/)
- [EMDB](https://eth-ait.github.io/emdb/)
- [COCO](https://cocodataset.org/#download)
- [SPEC-SYN](https://spec.is.tue.mpg.de/)
```shell
    data_inputs/
    â””â”€â”€ skel-evaluation-data/
        â”œâ”€â”€ 3dpw
        â”œâ”€â”€ coco
        â”œâ”€â”€ emdb
        â”œâ”€â”€ h36m-select
        â”œâ”€â”€ moyo
        â””â”€â”€ spec-syn
```
